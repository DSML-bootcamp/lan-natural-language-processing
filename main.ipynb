{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_30992\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab. Natural Lenguage Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speead up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1000, 2)\n",
      "Test set shape: (5964, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"data/kg_train.csv\", encoding='latin-1')\n",
    "test_data = pd.read_csv(\"data/kg_test.csv\", encoding='latin-1')\n",
    "\n",
    "# Reduce the training set \n",
    "train_data = train_data.head(1000)\n",
    "\n",
    "\n",
    "train_data.fillna(\"\", inplace=True)\n",
    "test_data.fillna(\"\", inplace=True)\n",
    "\n",
    "\n",
    "print(\"Training set shape:\", train_data.shape)\n",
    "print(\"Test set shape:\", test_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_30992\\4106381147.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  cleaned_html = BeautifulSoup(cleaned_html, \"html.parser\").get_text()\n",
      "C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_30992\\4106381147.py:13: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  cleaned_html = BeautifulSoup(cleaned_html, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  dear sir strictly a private business proposal ...      1\n",
      "1                                            will do      0\n",
      "2  noracheryl has emailed dozens of memos about h...      0\n",
      "3  dear sir2fmadam2c i know that this proposal mi...      1\n",
      "4                                                fyi      0\n",
      "                                                text\n",
      "0  usiness is for the fact that the deceased man ...\n",
      "1  they are happy to adjust to the afternoon i am...\n",
      "2  lael brainard was confirmed 7819 this afternoo...\n",
      "3  h friday march 26 2010 545 amsbwhoeopãâ¢ãâã...\n",
      "4  n dear good friendi am happy to inform you abo...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def clean_html(raw_html):\n",
    "    # Remove inline JavaScript/CSS\n",
    "    cleaned_html = re.sub(r'<(script|style).*?>.*?</\\1>', '', raw_html, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove HTML comments\n",
    "    cleaned_html = re.sub(r'<!--.*?-->', '', cleaned_html, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove remaining HTML tags\n",
    "    cleaned_html = BeautifulSoup(cleaned_html, \"html.parser\").get_text()\n",
    "    \n",
    "    return cleaned_html\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Clean HTML content\n",
    "    text = clean_html(text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    #lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to textcolumn\n",
    "train_data['text'] = train_data['text'].apply(preprocess_text)\n",
    "test_data['text'] = test_data['text'].apply(preprocess_text)\n",
    "\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  dear sir strictly private business proposal am...      1\n",
      "1                                            will do      0\n",
      "2  noracheryl has emailed dozens of memos about h...      0\n",
      "3  dear sirfmadamc know that this proposal might ...      1\n",
      "4                                                fyi      0\n",
      "                                                text\n",
      "0  usiness is for the fact that the deceased man ...\n",
      "1  they are happy to adjust to the afternoon am g...\n",
      "2  lael brainard was confirmed this afternoonmigu...\n",
      "3  h friday march amsbwhoeopã â ã â ã â rei have ...\n",
      "4  n dear good friendi am happy to inform you abo...\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing functions 2\n",
    "def preprocess_text_1(text):\n",
    "    # Clean HTML content\n",
    "    text = clean_html(text)\n",
    "    \n",
    "    # Remove all special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n",
    "    \n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    \n",
    "    # Remove prefixed 'b'\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to text column\n",
    "train_data['text'] = train_data['text'].apply(preprocess_text_1)\n",
    "test_data['text'] = test_data['text'].apply(preprocess_text_1)\n",
    "\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(remove_stopwords)\n",
    "test_data['text'] = test_data['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return lemmatized_text\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(lemmatize_text)\n",
    "test_data['text'] = test_data['text'].apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  dear sir strictly private business proposal mi...      1\n",
      "1                                                         0\n",
      "2  noracheryl emailed dozen memo haiti weekend pl...      0\n",
      "3  dear sirfmadamc know proposal might surprise e...      1\n",
      "4                                                fyi      0\n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in ham messages:\n",
      "â: 244\n",
      "u: 119\n",
      "pm: 115\n",
      "would: 106\n",
      "state: 103\n",
      "president: 94\n",
      "call: 91\n",
      "time: 84\n",
      "percent: 77\n",
      "secretary: 76\n",
      "\n",
      "Top 10 words in spam messages:\n",
      "money: 920\n",
      "account: 795\n",
      "bank: 745\n",
      "fund: 705\n",
      "u: 550\n",
      "business: 474\n",
      "transaction: 416\n",
      "country: 407\n",
      "transfer: 393\n",
      "million: 386\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tokenize the text data\n",
    "ham_messages = train_data[train_data['label'] == 0]['text'].apply(lambda x: x.split())\n",
    "spam_messages = train_data[train_data['label'] == 1]['text'].apply(lambda x: x.split())\n",
    "\n",
    "# Count the occurrences of each word\n",
    "ham_word_counts = Counter(word for message in ham_messages for word in message)\n",
    "spam_word_counts = Counter(word for message in spam_messages for word in message)\n",
    "\n",
    "#top 10 words\n",
    "top_10_ham_words = ham_word_counts.most_common(10)\n",
    "top_10_spam_words = spam_word_counts.most_common(10)\n",
    "\n",
    "print(\"Top 10 words in ham messages:\")\n",
    "for word, count in top_10_ham_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 words in spam messages:\")\n",
    "for word, count in top_10_spam_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  money_mark  \\\n",
      "0  dear sir strictly private business proposal mi...      1           1   \n",
      "1                                                         0           1   \n",
      "2  noracheryl emailed dozen memo haiti weekend pl...      0           1   \n",
      "3  dear sirfmadamc know proposal might surprise e...      1           1   \n",
      "4                                                fyi      0           1   \n",
      "\n",
      "   suspicious_words  text_len  \n",
      "0                 1      1504  \n",
      "1                 0         0  \n",
      "2                 0       110  \n",
      "3                 1      1380  \n",
      "4                 0         3  \n"
     ]
    }
   ],
   "source": [
    "money_symbol_list = [\"euro\", \"dollar\", \"pound\", \"€\", \"$\"]\n",
    "suspicious_words = [\"free\", \"cheap\", \"sex\", \"money\", \"account\", \"bank\", \"fund\", \"transfer\", \"transaction\", \"win\", \"deposit\", \"password\"]\n",
    "\n",
    "# Convert the lists into regular expression patterns\n",
    "money_symbol_pattern = \"|\".join(money_symbol_list)\n",
    "suspicious_words_pattern = \"|\".join(suspicious_words)\n",
    "\n",
    "# Define functions to check for the moneu symbols and sus words\n",
    "def has_money_symbol(text):\n",
    "    return int(bool(re.search(money_symbol_pattern, text)))\n",
    "\n",
    "def has_suspicious_words(text):\n",
    "    return int(bool(re.search(suspicious_words_pattern, text)))\n",
    "\n",
    "# Add indicators to the original dataframe\n",
    "train_data['money_mark'] = train_data['text'].apply(has_money_symbol)\n",
    "train_data['suspicious_words'] = train_data['text'].apply(has_suspicious_words)\n",
    "train_data['text_len'] = train_data['text'].apply(len)\n",
    "\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "#fit the vectoriser to the trian_data and transform\n",
    "X = vectorizer.fit_transform(train_data)\n",
    "\n",
    "#feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Representation:\n",
      "[[0 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 1]]\n",
      "\n",
      "Feature Names:\n",
      "['label' 'money_mark' 'suspicious_words' 'text' 'text_len']\n"
     ]
    }
   ],
   "source": [
    "print(\"Bag-of-Words Representation:\")\n",
    "print(X.toarray())\n",
    "\n",
    "print(\"\\nFeature Names:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the vectorized dataset: (5, 5)\n"
     ]
    }
   ],
   "source": [
    "# shape of the vetorized dataset\n",
    "print(\"Shape of the vectorized dataset:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data['text'])\n",
    "X_test = vectorizer.transform(test_data['text'])\n",
    "\n",
    "# Define labels\n",
    "y_train = train_data['label']\n",
    "\n",
    "# Create a Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_30992\\4106381147.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  cleaned_html = BeautifulSoup(cleaned_html, \"html.parser\").get_text()\n",
      "C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_30992\\4106381147.py:13: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  cleaned_html = BeautifulSoup(cleaned_html, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9471919530595139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95       674\n",
      "           1       0.92      0.96      0.94       519\n",
      "\n",
      "    accuracy                           0.95      1193\n",
      "   macro avg       0.95      0.95      0.95      1193\n",
      "weighted avg       0.95      0.95      0.95      1193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "data = pd.read_csv(\"data/kg_train.csv\", encoding='latin-1')\n",
    "\n",
    "\n",
    "#data preprocess here\n",
    "data.fillna(\"\", inplace=True)\n",
    "data['text'] = data['text'].apply(preprocess_text).apply(preprocess_text_1).apply(remove_stopwords).apply(lemmatize_text)\n",
    "\n",
    "\n",
    "X = data['text'] \n",
    "y = data['label'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = CountVectorizer()  # You can experiment with different vectorizers\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Create and train the MultinomialNB classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_vectorized)\n",
    "\n",
    "#eval\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1x/gfq_5w6527q1mtcmjs8xlyxr0000gn/T/ipykernel_37819/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab. Natural Lenguage Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speead up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 800\n",
      "Test set size: 200\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "X = data['text']\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gerardroca/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gerardroca/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gerardroca/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1x/gfq_5w6527q1mtcmjs8xlyxr0000gn/T/ipykernel_37819/1733798072.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_no_comments, \"html.parser\")\n",
      "/var/folders/1x/gfq_5w6527q1mtcmjs8xlyxr0000gn/T/ipykernel_37819/1733798072.py:9: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_no_comments, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29     ----------- REGARDS, MR NELSON SMITH.KINDLY RE...\n",
      "535    I have not been able to reach oscar this am. W...\n",
      "695    ; Huma Abedin B6I'm checking with Pat on the 5...\n",
      "557    I can have it announced here on Monday - can't...\n",
      "836        BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(html: str) -> str:\n",
    "    html_no_js = re.sub(r'<script.*?>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)    \n",
    "    html_no_css = re.sub(r'<style.*?>.*?</style>', '', html_no_js, flags=re.DOTALL | re.IGNORECASE)\n",
    "    html_no_comments = re.sub(r'<!--.*?-->', '', html_no_css, flags=re.DOTALL | re.IGNORECASE)\n",
    "    soup = BeautifulSoup(html_no_comments, \"html.parser\")\n",
    "    html_no_tags = soup.get_text(separator=\" \")\n",
    "    text_only = re.sub(r'<[^>]+>', ' ', html_no_tags)\n",
    "    return text_only\n",
    "\n",
    "X_train = X_train.apply(clean_html)\n",
    "print(X_train.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1x/gfq_5w6527q1mtcmjs8xlyxr0000gn/T/ipykernel_37819/1733798072.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_no_comments, \"html.parser\")\n",
      "/var/folders/1x/gfq_5w6527q1mtcmjs8xlyxr0000gn/T/ipykernel_37819/1733798072.py:9: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_no_comments, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    dear sir strictly a private business proposal ...\n",
      "1                                              will do\n",
      "2    noracheryl has emailed dozens of memos about h...\n",
      "3    dear sirfmadamc i know that this proposal migh...\n",
      "4                                                  fyi\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "def clean_text(text:str) -> str:\n",
    "    text_no_spc_char = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text_no_numbers = re.sub(r'\\d+', '', text_no_spc_char)\n",
    "    text_no_single_char = re.sub(r'^[a-zA-Z]\\s+', '', text_no_numbers)\n",
    "    text_no_single_char_start = re.sub(r'^[a-zA-Z]\\s+', '', text_no_single_char)\n",
    "    text_no_multiple_spaces = re.sub(r'\\s+', ' ', text_no_single_char_start).strip()\n",
    "    text_removed_prefixed_b = text_no_multiple_spaces.lstrip('b')\n",
    "    text_lowercase = text_removed_prefixed_b.lower()\n",
    "    return text_lowercase\n",
    "\n",
    "data['cleaned_html'] = data['text'].apply(clean_html)\n",
    "data['cleaned_text'] = data['cleaned_html'].apply(clean_text)\n",
    "print(data['cleaned_text'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    dear sir strictly private business proposal mi...\n",
      "1                                                     \n",
      "2    noracheryl emailed dozens memos haiti weekend ...\n",
      "3    dear sirfmadamc know proposal might surprise e...\n",
      "4                                                  fyi\n",
      "Name: no_stopwords_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "def remove_stopwords(text:str) -> str:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_no_stopwords = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text_no_stopwords\n",
    "\n",
    "data['no_stopwords_text'] = data['cleaned_text'].apply(remove_stopwords)\n",
    "\n",
    "print(data['no_stopwords_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    dear sir strictly private business proposal mi...\n",
      "1                                                     \n",
      "2    noracheryl emailed dozen memo haiti weekend pl...\n",
      "3    dear sirfmadamc know proposal might surprise e...\n",
      "4                                                  fyi\n",
      "Name: final_cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "def lemmatize_text(text: str) -> str:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return lemmatized_text\n",
    "\n",
    "data['final_cleaned_text'] = data['no_stopwords_text'].apply(lemmatize_text)\n",
    "\n",
    "print(data['final_cleaned_text'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in ham messages: [('u', 135), ('pm', 115), ('would', 106), ('state', 103), ('president', 94), ('call', 91), ('time', 84), ('percent', 77), ('secretary', 76), ('work', 73)]\n",
      "Top 10 words in spam messages: [('money', 926), ('account', 801), ('bank', 748), ('fund', 709), ('u', 566), ('business', 476), ('transaction', 426), ('country', 414), ('transfer', 399), ('million', 389)]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from collections import Counter\n",
    "import string\n",
    "# Separate ham and spam messages\n",
    "ham_messages = data[data['label'] == 0]['final_cleaned_text']\n",
    "spam_messages = data[data['label'] == 1]['final_cleaned_text']\n",
    "\n",
    "def get_top_n_words(text_series, n=10):\n",
    "    all_words = ' '.join(text_series).split()\n",
    "    word_counts = Counter(all_words)\n",
    "    return word_counts.most_common(n)\n",
    "\n",
    "top_ham_words = get_top_n_words(ham_messages)\n",
    "top_spam_words = get_top_n_words(spam_messages)\n",
    "\n",
    "print(\"Top 10 words in ham messages:\", top_ham_words)\n",
    "print(\"Top 10 words in spam messages:\", top_spam_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_html</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>final_cleaned_text</th>\n",
       "      <th>no_stopwords_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>dear sir strictly a private business proposal ...</td>\n",
       "      <td>dear sir strictly private business proposal mi...</td>\n",
       "      <td>dear sir strictly private business proposal mi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "      <td>Will do.</td>\n",
       "      <td>will do</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>noracheryl has emailed dozens of memos about h...</td>\n",
       "      <td>noracheryl emailed dozen memo haiti weekend pl...</td>\n",
       "      <td>noracheryl emailed dozens memos haiti weekend ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>dear sirfmadamc i know that this proposal migh...</td>\n",
       "      <td>dear sirfmadamc know proposal might surprise e...</td>\n",
       "      <td>dear sirfmadamc know proposal might surprise e...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>fyi</td>\n",
       "      <td>fyi</td>\n",
       "      <td>fyi</td>\n",
       "      <td>fyi</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1   \n",
       "1                                           Will do.      0   \n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0   \n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1   \n",
       "4                                                fyi      0   \n",
       "\n",
       "                                        cleaned_html  \\\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
       "1                                           Will do.   \n",
       "2  Nora--Cheryl has emailed dozens of memos about...   \n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
       "4                                                fyi   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  dear sir strictly a private business proposal ...   \n",
       "1                                            will do   \n",
       "2  noracheryl has emailed dozens of memos about h...   \n",
       "3  dear sirfmadamc i know that this proposal migh...   \n",
       "4                                                fyi   \n",
       "\n",
       "                                  final_cleaned_text  \\\n",
       "0  dear sir strictly private business proposal mi...   \n",
       "1                                                      \n",
       "2  noracheryl emailed dozen memo haiti weekend pl...   \n",
       "3  dear sirfmadamc know proposal might surprise e...   \n",
       "4                                                fyi   \n",
       "\n",
       "                                   no_stopwords_text  money_mark  \\\n",
       "0  dear sir strictly private business proposal mi...           1   \n",
       "1                                                              1   \n",
       "2  noracheryl emailed dozens memos haiti weekend ...           1   \n",
       "3  dear sirfmadamc know proposal might surprise e...           1   \n",
       "4                                                fyi           1   \n",
       "\n",
       "   suspicious_words  text_len  \n",
       "0                 1      1504  \n",
       "1                 0         0  \n",
       "2                 0       110  \n",
       "3                 1      1382  \n",
       "4                 0         3  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_symbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"â‚¬\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data['money_mark'] = data['final_cleaned_text'].str.contains(money_symbol_list, case=False, regex=True) * 1\n",
    "data['suspicious_words'] = data['final_cleaned_text'].str.contains(suspicious_words, case=False, regex=True) * 1\n",
    "data['text_len'] = data['final_cleaned_text'].apply(lambda x: len(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the TF-IDF vectorized dataset: (1000, 19006)\n",
      "   aac  aaclocated  aae  aag  aaronovitchon  abacha  abachabefore  abachac  \\\n",
      "0  0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
      "1  0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
      "2  0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
      "3  0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
      "4  0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
      "\n",
      "   abachace  abachaco  ...  zongo  zongothe  zuhair  \\\n",
      "0       0.0       0.0  ...    0.0       0.0     0.0   \n",
      "1       0.0       0.0  ...    0.0       0.0     0.0   \n",
      "2       0.0       0.0  ...    0.0       0.0     0.0   \n",
      "3       0.0       0.0  ...    0.0       0.0     0.0   \n",
      "4       0.0       0.0  ...    0.0       0.0     0.0   \n",
      "\n",
      "   zulatoebikozulatonetscapeenet  zulatoffffffffffffffffff      zuma    zumac  \\\n",
      "0                            0.0                       0.0  0.000000  0.00000   \n",
      "1                            0.0                       0.0  0.000000  0.00000   \n",
      "2                            0.0                       0.0  0.000000  0.00000   \n",
      "3                            0.0                       0.0  0.089947  0.10303   \n",
      "4                            0.0                       0.0  0.000000  0.00000   \n",
      "\n",
      "   zumadirector    zumae  zurich  \n",
      "0           0.0  0.00000     0.0  \n",
      "1           0.0  0.00000     0.0  \n",
      "2           0.0  0.00000     0.0  \n",
      "3           0.0  0.10303     0.0  \n",
      "4           0.0  0.00000     0.0  \n",
      "\n",
      "[5 rows x 19006 columns]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(data['final_cleaned_text'])\n",
    "print(\"Shape of the TF-IDF vectorized dataset:\", X_tfidf.shape)\n",
    "\n",
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(X_tfidf_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95       125\n",
      "           1       0.88      0.99      0.93        75\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.94      0.95      0.94       200\n",
      "weighted avg       0.95      0.94      0.95       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

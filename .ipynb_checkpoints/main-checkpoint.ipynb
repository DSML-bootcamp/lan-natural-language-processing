{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab. Natural Lenguage Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
       "1                                           Will do.      0\n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
       "4                                                fyi      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emin.sen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<nltk.stem.snowball.SnowballStemmer at 0x1ca346f2f50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "snowball = SnowballStemmer('english')\n",
    "snowball\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_js(html: str) -> str:\n",
    "    html_no_js = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)    \n",
    "    return html_no_js\n",
    "\n",
    "def remove_inline_css(html: str) -> str:    \n",
    "    html_no_inline_css = re.sub(r'style=\"[^\"]*\"', '', html, flags=re.IGNORECASE)\n",
    "    return html_no_inline_css\n",
    "\n",
    "def remove_html_comments(html: str) -> str:\n",
    "    html_no_comments = re.sub(r'<!--.*?-->', '', html, flags=re.DOTALL | re.IGNORECASE)\n",
    "    return html_no_comments\n",
    "\n",
    "def clean_html(html : str) -> str:\n",
    "    no_js = remove_js(html)\n",
    "    no_css = remove_inline_css(no_js)\n",
    "    no_html = remove_html_comments(no_css)\n",
    "    return no_html\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emin.sen\\AppData\\Local\\Temp\\ipykernel_16536\\3528488058.py:7: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html, 'html.parser')\n",
      "C:\\Users\\emin.sen\\AppData\\Local\\Temp\\ipykernel_16536\\3528488058.py:7: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(html, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def remove_inline_js_css(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    for script_or_style in soup(['script', 'style']):\n",
    "        script_or_style.decompose()\n",
    "    \n",
    "    return soup.get_text()\n",
    "\n",
    "data['cleaned_text'] = data['text'].apply(remove_inline_js_css)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    \n",
    "    text = re.sub(r'^\\w\\s+', '', text)\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\bb\\b', '', text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "data['cleaned_text'] = data['cleaned_text'].apply(preprocess_text)\n",
    "\n",
    "if not data['cleaned_text'].empty:\n",
    "    # Create a TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    # Fit and transform the cleaned text\n",
    "    tfidf_matrix = vectorizer.fit_transform(data['cleaned_text'])\n",
    "\n",
    "    # Get the feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['aac' 'aaclocated' 'aae' ... 'zumadirector' 'zumae' 'zurich']\n",
      "TF-IDF Matrix: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "TF-IDF Matrix after additional preprocessing steps:\n",
      "     aac  aaclocated  aae  aag  aaronovitchon  abacha  abachabefore  abachac  \\\n",
      "0    0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
      "1    0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
      "2    0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
      "3    0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
      "4    0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
      "..   ...         ...  ...  ...            ...     ...           ...      ...   \n",
      "995  0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
      "996  0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
      "997  0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
      "998  0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
      "999  0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
      "\n",
      "     abachace  abachaco  ...  zongo  zongothe  zuhair  \\\n",
      "0         0.0       0.0  ...    0.0       0.0     0.0   \n",
      "1         0.0       0.0  ...    0.0       0.0     0.0   \n",
      "2         0.0       0.0  ...    0.0       0.0     0.0   \n",
      "3         0.0       0.0  ...    0.0       0.0     0.0   \n",
      "4         0.0       0.0  ...    0.0       0.0     0.0   \n",
      "..        ...       ...  ...    ...       ...     ...   \n",
      "995       0.0       0.0  ...    0.0       0.0     0.0   \n",
      "996       0.0       0.0  ...    0.0       0.0     0.0   \n",
      "997       0.0       0.0  ...    0.0       0.0     0.0   \n",
      "998       0.0       0.0  ...    0.0       0.0     0.0   \n",
      "999       0.0       0.0  ...    0.0       0.0     0.0   \n",
      "\n",
      "     zulatoebikozulatonetscapeenet  zulatoffffffffffffffffff      zuma  \\\n",
      "0                              0.0                       0.0  0.000000   \n",
      "1                              0.0                       0.0  0.000000   \n",
      "2                              0.0                       0.0  0.000000   \n",
      "3                              0.0                       0.0  0.091673   \n",
      "4                              0.0                       0.0  0.000000   \n",
      "..                             ...                       ...       ...   \n",
      "995                            0.0                       0.0  0.000000   \n",
      "996                            0.0                       0.0  0.000000   \n",
      "997                            0.0                       0.0  0.000000   \n",
      "998                            0.0                       0.0  0.000000   \n",
      "999                            0.0                       0.0  0.000000   \n",
      "\n",
      "        zumac  zumadirector     zumae  zurich  \n",
      "0    0.000000           0.0  0.000000     0.0  \n",
      "1    0.000000           0.0  0.000000     0.0  \n",
      "2    0.000000           0.0  0.000000     0.0  \n",
      "3    0.105008           0.0  0.105008     0.0  \n",
      "4    0.000000           0.0  0.000000     0.0  \n",
      "..        ...           ...       ...     ...  \n",
      "995  0.000000           0.0  0.000000     0.0  \n",
      "996  0.000000           0.0  0.000000     0.0  \n",
      "997  0.000000           0.0  0.000000     0.0  \n",
      "998  0.000000           0.0  0.000000     0.0  \n",
      "999  0.000000           0.0  0.000000     0.0  \n",
      "\n",
      "[1000 rows x 19930 columns]\n"
     ]
    }
   ],
   "source": [
    "data['cleaned_text'] = data['cleaned_text'].apply(preprocess_text)\n",
    "\n",
    "# Ensure cleaned_text is not empty\n",
    "if not data['cleaned_text'].empty:\n",
    "    # Create a TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    # Fit and transform the cleaned text\n",
    "    tfidf_matrix = vectorizer.fit_transform(data['cleaned_text'])\n",
    "\n",
    "    # Get the feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Print the TF-IDF scores\n",
    "    print(f\"Feature names: {feature_names}\")\n",
    "    print(f\"TF-IDF Matrix: {tfidf_matrix.toarray()}\")\n",
    "\n",
    "    # Additional preprocessing steps on TF-IDF result\n",
    "    # Convert TF-IDF matrix to DataFrame\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "    # Remove all special characters from columns\n",
    "    tfidf_df.columns = [re.sub(r'[^a-zA-Z\\s]', '', col) for col in tfidf_df.columns]\n",
    "\n",
    "    # Remove numbers from columns\n",
    "    tfidf_df.columns = [re.sub(r'\\d+', '', col) for col in tfidf_df.columns]\n",
    "\n",
    "    # Remove all single characters from columns\n",
    "    tfidf_df = tfidf_df.loc[:, tfidf_df.columns.map(len) > 1]\n",
    "\n",
    "    # Remove single characters from the start of columns\n",
    "    tfidf_df.columns = [re.sub(r'^\\w\\s+', '', col) for col in tfidf_df.columns]\n",
    "\n",
    "    # Substitute multiple spaces with a single space\n",
    "    tfidf_df.columns = [re.sub(r'\\s+', ' ', col) for col in tfidf_df.columns]\n",
    "\n",
    "    # Remove prefixed 'b' from columns\n",
    "    tfidf_df.columns = [re.sub(r'\\bb\\b', '', col) for col in tfidf_df.columns]\n",
    "\n",
    "    # Convert column names to lowercase\n",
    "    tfidf_df.columns = [col.lower() for col in tfidf_df.columns]\n",
    "\n",
    "    print(\"TF-IDF Matrix after additional preprocessing steps:\")\n",
    "    print(tfidf_df)\n",
    "else:\n",
    "    print(\"The cleaned text is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "punk = string.punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_html(html):\n",
    "    text = re.sub(punk, '', html)\n",
    "    return text\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      9\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mENGLISH_STOP_WORDS\n\u001b[1;32m---> 10\u001b[0m tfidf_df \u001b[38;5;241m=\u001b[39m tfidf_df[[col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m tfidf_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_df' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "tfidf_df = tfidf_df[[col for col in tfidf_df.columns if col not in stop_words]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aac</th>\n",
       "      <th>aaclocated</th>\n",
       "      <th>aae</th>\n",
       "      <th>aag</th>\n",
       "      <th>aaronovitchon</th>\n",
       "      <th>abacha</th>\n",
       "      <th>abachabefore</th>\n",
       "      <th>abachac</th>\n",
       "      <th>abachace</th>\n",
       "      <th>abachaco</th>\n",
       "      <th>...</th>\n",
       "      <th>zongo</th>\n",
       "      <th>zongothe</th>\n",
       "      <th>zuhair</th>\n",
       "      <th>zulatoebikozulatonetscapeenet</th>\n",
       "      <th>zulatoffffffffffffffffff</th>\n",
       "      <th>zuma</th>\n",
       "      <th>zumac</th>\n",
       "      <th>zumadirector</th>\n",
       "      <th>zumae</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091673</td>\n",
       "      <td>0.105008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105008</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 19930 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aac  aaclocated  aae  aag  aaronovitchon  abacha  abachabefore  abachac  \\\n",
       "0  0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
       "1  0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
       "2  0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
       "3  0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
       "4  0.0         0.0  0.0  0.0            0.0     0.0           0.0      0.0   \n",
       "\n",
       "   abachace  abachaco  ...  zongo  zongothe  zuhair  \\\n",
       "0       0.0       0.0  ...    0.0       0.0     0.0   \n",
       "1       0.0       0.0  ...    0.0       0.0     0.0   \n",
       "2       0.0       0.0  ...    0.0       0.0     0.0   \n",
       "3       0.0       0.0  ...    0.0       0.0     0.0   \n",
       "4       0.0       0.0  ...    0.0       0.0     0.0   \n",
       "\n",
       "   zulatoebikozulatonetscapeenet  zulatoffffffffffffffffff      zuma  \\\n",
       "0                            0.0                       0.0  0.000000   \n",
       "1                            0.0                       0.0  0.000000   \n",
       "2                            0.0                       0.0  0.000000   \n",
       "3                            0.0                       0.0  0.091673   \n",
       "4                            0.0                       0.0  0.000000   \n",
       "\n",
       "      zumac  zumadirector     zumae  zurich  \n",
       "0  0.000000           0.0  0.000000     0.0  \n",
       "1  0.000000           0.0  0.000000     0.0  \n",
       "2  0.000000           0.0  0.000000     0.0  \n",
       "3  0.105008           0.0  0.105008     0.0  \n",
       "4  0.000000           0.0  0.000000     0.0  \n",
       "\n",
       "[5 rows x 19930 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_columns = [lemmatizer.lemmatize(col) for col in tfidf_df.columns]\n",
    "tfidf_df.columns = lemmatized_columns\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (558, 7066), indices imply (558, 14336)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m spam_bow \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(spam_messages)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Sum up the counts of each vocabulary word\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m ham_word_counts \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(ham_bow\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     32\u001b[0m spam_word_counts \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(spam_bow\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Get the top 10 words in ham and spam messages\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:782\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    771\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    772\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    773\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    779\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[0;32m    780\u001b[0m         )\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 782\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    783\u001b[0m             data,\n\u001b[0;32m    784\u001b[0m             index,\n\u001b[0;32m    785\u001b[0m             columns,\n\u001b[0;32m    786\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    787\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    788\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    789\u001b[0m         )\n\u001b[0;32m    791\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    334\u001b[0m )\n\u001b[1;32m--> 336\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (558, 7066), indices imply (558, 14336)"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Separate ham and spam messages\n",
    "ham_messages = data[data['label'] == 0]['cleaned_text']\n",
    "spam_messages = data[data['label'] == 1]['cleaned_text']\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Apply lemmatization\n",
    "ham_messages = ham_messages.apply(tokenize_and_lemmatize)\n",
    "spam_messages = spam_messages.apply(tokenize_and_lemmatize)\n",
    "\n",
    "# Vectorize the text using Bag of Words (BoW)\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "\n",
    "# Vectorize the text using Bag of Words (BoW)\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "\n",
    "# Fit and transform the ham and spam messages\n",
    "ham_bow = vectorizer.fit_transform(ham_messages)\n",
    "spam_bow = vectorizer.fit_transform(spam_messages)\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "ham_word_counts = pd.DataFrame(ham_bow.toarray(), columns=vectorizer.get_feature_names_out()).sum(axis=0)\n",
    "spam_word_counts = pd.DataFrame(spam_bow.toarray(), columns=vectorizer.get_feature_names_out()).sum(axis=0)\n",
    "\n",
    "# Get the top 10 words in ham and spam messages\n",
    "top_10_ham_words = ham_word_counts.sort_values(ascending=False).head(10)\n",
    "top_10_spam_words = spam_word_counts.sort_values(ascending=False).head(10)\n",
    "\n",
    "print(\"Top 10 words in ham messages:\")\n",
    "print(top_10_ham_words)\n",
    "\n",
    "print(\"\\nTop 10 words in spam messages:\")\n",
    "print(top_10_spam_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m money_simbol_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuro\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdollar\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpound\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m€\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      2\u001b[0m suspicious_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfree\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoney\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccount\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbank\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfund\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransfer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransaction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeposit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoney_mark\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(money_simbol_list)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      5\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuspicious_words\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(suspicious_words)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      6\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_len\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x)) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data['money_mark'] = data['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data['suspicious_words'] = data['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data['text_len'] = data['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(tfidf_df)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_df' is not defined"
     ]
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the dataset\n",
    "X = vectorizer.fit_transform(tfidf_df['text'])\n",
    "y = tfidf_df['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
